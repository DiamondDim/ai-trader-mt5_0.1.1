import pandas as pd
import numpy as np
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, f1_score
from sklearn.preprocessing import StandardScaler
import joblib
import os
from typing import Optional
import warnings

warnings.filterwarnings('ignore')


class ModelBuilder:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∏ –æ–±—É—á–µ–Ω–∏—è ML –º–æ–¥–µ–ª–µ–π"""

    def __init__(self, config: dict):
        self.config = config
        self.model = None
        self.scaler = StandardScaler()
        self.feature_importance_ = None

    def create_model(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        model_config = self.config.get('model', {})
        model_type = model_config.get('model_type', 'GradientBoosting')

        print(f"ü§ñ –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: {model_type}")

        if model_type == "GradientBoosting":
            model_params = model_config.get('model_params', {})
            self.model = GradientBoostingClassifier(
                n_estimators=model_params.get('n_estimators', 200),  # –£–≤–µ–ª–∏—á–∏–ª–∏
                max_depth=model_params.get('max_depth', 4),  # –£–≤–µ–ª–∏—á–∏–ª–∏
                learning_rate=model_params.get('learning_rate', 0.05),  # –£–º–µ–Ω—å—à–∏–ª–∏
                min_samples_split=model_params.get('min_samples_split', 50),  # –î–æ–±–∞–≤–∏–ª–∏
                min_samples_leaf=model_params.get('min_samples_leaf', 20),  # –î–æ–±–∞–≤–∏–ª–∏
                subsample=model_params.get('subsample', 0.8),  # –î–æ–±–∞–≤–∏–ª–∏
                random_state=model_params.get('random_state', 42)
            )
        elif model_type == "RandomForest":
            model_params = model_config.get('model_params', {})
            self.model = RandomForestClassifier(
                n_estimators=model_params.get('n_estimators', 200),
                max_depth=model_params.get('max_depth', 6),
                min_samples_split=model_params.get('min_samples_split', 20),
                min_samples_leaf=model_params.get('min_samples_leaf', 10),
                random_state=model_params.get('random_state', 42),
                n_jobs=-1
            )
        else:
            # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π GradientBoosting
            self.model = GradientBoostingClassifier(
                n_estimators=200,
                max_depth=4,
                learning_rate=0.05,
                min_samples_split=50,
                min_samples_leaf=20,
                subsample=0.8,
                random_state=42
            )

        return self.model

    def train_model(self, data: pd.DataFrame, symbol: str, feature_engineer) -> Optional[str]:
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ–∏—á
            print("üìä –ù–∞—á–∞–ª–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...")
            features_df = feature_engineer.prepare_features(data, symbol)
            feature_names = feature_engineer.get_feature_names()

            if len(features_df) == 0:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
                return None

            # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π (–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ü–µ–Ω—ã)
            features_df = self._create_improved_target(features_df)

            # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN
            initial_samples = len(features_df)
            features_df = features_df.dropna()
            final_samples = len(features_df)

            print(f"üìà –î–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {final_samples}/{initial_samples} samples")

            if len(features_df) < 200:
                print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
                return None

            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
            X = features_df[feature_names]
            y = features_df['target']

            print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(X)} samples, {len(feature_names)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

            # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            print("üîß –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
            X_scaled = self.scaler.fit_transform(X)
            X = pd.DataFrame(X_scaled, columns=feature_names, index=X.index)

            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
            test_size = self.config.get('ml', {}).get('test_size', 0.2)  # –£–º–µ–Ω—å—à–∏–ª–∏ test_size
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=test_size, random_state=42, shuffle=False
            )

            print(f"üìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö: train={len(X_train)}, test={len(X_test)}")

            # –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤
            class_ratio = y_train.mean()
            print(f"üìä –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: {class_ratio:.3f} (1) / {1 - class_ratio:.3f} (0)")

            # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            model = self.create_model()

            print("üéØ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏...")
            model.fit(X_train, y_train)

            # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
            cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
            print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. CV Accuracy: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}")

            # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            y_pred = model.predict(X_test)
            y_pred_proba = model.predict_proba(X_test)
            test_accuracy = accuracy_score(y_test, y_pred)
            test_f1 = f1_score(y_test, y_pred, average='weighted')

            print(f"üß™ –¢–µ—Å—Ç–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {test_accuracy:.4f}")
            print(f"üéØ Test F1-Score: {test_f1:.4f}")

            # –ê–Ω–∞–ª–∏–∑ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
            confidence_mean = y_pred_proba.max(axis=1).mean()
            confidence_std = y_pred_proba.max(axis=1).std()
            print(f"üìä –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence_mean:.4f} ¬± {confidence_std:.4f}")

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            if hasattr(model, 'feature_importances_'):
                self.feature_importance_ = pd.DataFrame({
                    'feature': feature_names,
                    'importance': model.feature_importances_
                }).sort_values('importance', ascending=False)

                print("\nüèÜ –¢–æ–ø-15 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
                for i, row in self.feature_importance_.head(15).iterrows():
                    print(f"   {row['feature']}: {row['importance']:.4f}")

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ scaler
            model_path = self.config.get('ml', {}).get('model_path', 'models/trained_model.pkl')
            scaler_path = model_path.replace('.pkl', '_scaler.pkl')
            os.makedirs(os.path.dirname(model_path), exist_ok=True)

            joblib.dump(model, model_path)
            joblib.dump(self.scaler, scaler_path)
            print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {model_path}")
            print(f"üíæ Scaler —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {scaler_path}")

            self.model = model
            return model_path

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
            import traceback
            traceback.print_exc()
            return None

    def _create_improved_target(self, df: pd.DataFrame, horizon: int = 3) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π"""
        # –£–º–µ–Ω—å—à–∞–µ–º –≥–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –±–æ–ª—å—à–µ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        df['future_price'] = df['close'].shift(-horizon)
        df['price_change'] = (df['future_price'] - df['close']) / df['close']

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Ä–æ–≥ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Å–ª–∞–±—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π
        threshold = 0.0005  # 0.05% –ø–æ—Ä–æ–≥
        df['target'] = 1  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø–æ–∫—É–ø–∞—Ç—å

        # –ü—Ä–æ–¥–∞–≤–∞—Ç—å –µ—Å–ª–∏ –ø–∞–¥–µ–Ω–∏–µ –±–æ–ª—å—à–µ –ø–æ—Ä–æ–≥–∞
        df.loc[df['price_change'] < -threshold, 'target'] = 0
        # –î–µ—Ä–∂–∞—Ç—å –µ—Å–ª–∏ –¥–≤–∏–∂–µ–Ω–∏–µ –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –ø–æ—Ä–æ–≥–∞
        df.loc[(df['price_change'] >= -threshold) & (df['price_change'] <= threshold), 'target'] = 2

        # –£–¥–∞–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ horizon —Å—Ç—Ä–æ–∫ (—É –Ω–∏—Ö –Ω–µ—Ç future_price)
        df = df[:-horizon]

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å–∏–ª—å–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã (—É–±–∏—Ä–∞–µ–º target=2)
        df = df[df['target'] != 2]

        print(f"üéØ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π:")
        target_counts = df['target'].value_counts()
        for target_val, count in target_counts.items():
            direction = "BUY" if target_val == 1 else "SELL"
            percentage = count / len(df) * 100
            print(f"   {direction} ({target_val}): {count} samples ({percentage:.1f}%)")

        return df

    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ scaler"""
        try:
            model_path = self.config.get('ml', {}).get('model_path', 'models/trained_model.pkl')
            scaler_path = model_path.replace('.pkl', '_scaler.pkl')

            if not os.path.exists(model_path):
                print(f"‚ùå –§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {model_path}")
                return None

            self.model = joblib.load(model_path)

            if os.path.exists(scaler_path):
                self.scaler = joblib.load(scaler_path)
                print(f"‚úÖ Scaler –∑–∞–≥—Ä—É–∂–µ–Ω: {scaler_path}")

            print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_path}")
            return self.model

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            return None

    def predict(self, features):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        if self.model is None:
            print("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return None

        try:
            # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            features_scaled = self.scaler.transform(features)
            prediction = self.model.predict(features_scaled)
            probabilities = self.model.predict_proba(features_scaled)
            return prediction, probabilities
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            return None
